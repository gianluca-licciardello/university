{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13973490,"sourceType":"datasetVersion","datasetId":8908501}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"NZMH92WHKkua","cell_type":"markdown","source":"# NMT Homework (Self-Contained): EN→DE\n\nTrain a translation model (English→German), measure perplexity and BLEU, save a checkpoint, and optionally export predictions for ML‑Arena.\n\nFocus: experiment with architectures (LSTM w/ attention, Transformer, decoding strategies) — not boilerplate. Core evaluation functions are provided to ensure consistent scoring across students.\n\nData: the course staff provides `dataset_splits/` in the repo root. No additional setup is needed for data.","metadata":{"id":"NZMH92WHKkua"}},{"id":"7A4xIBrUKkui","cell_type":"markdown","source":"## 0. Setup\nUse `install.sh` or `pip install -r requirements.txt` to set up.","metadata":{"id":"7A4xIBrUKkui"}},{"id":"xPKALnutKkuj","cell_type":"code","source":"import torch, sys, os, math, random\nprint('PyTorch version:', torch.__version__)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\ntry: sys.stdout.reconfigure(line_buffering=True)\nexcept Exception: pass","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5321,"status":"ok","timestamp":1764770546734,"user":{"displayName":"Gianluca Licciardello","userId":"04706827945424123627"},"user_tz":-60},"id":"xPKALnutKkuj","outputId":"133945a9-a289-47db-b575-1cbd8b5d03b7","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:26:53.691376Z","iopub.execute_input":"2025-12-03T14:26:53.691715Z","iopub.status.idle":"2025-12-03T14:26:53.696871Z","shell.execute_reply.started":"2025-12-03T14:26:53.691691Z","shell.execute_reply":"2025-12-03T14:26:53.696121Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.6.0+cu124\nUsing device: cuda\n","output_type":"stream"}],"execution_count":3},{"id":"cgkDzF_4Kkul","cell_type":"markdown","source":"## 1. Shared Utilities (no external imports)\nTokenization, vocabulary, dataset, collate, and fixed evaluation (PPL, NLL, BLEU).","metadata":{"id":"cgkDzF_4Kkul"}},{"id":"pGWPedS7Kkum","cell_type":"code","source":"from typing import List, Tuple, Dict, Iterable\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\ndef set_seed(seed: int = 42):\n    random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n\nSPECIAL_TOKENS = {'pad': '<pad>', 'sos': '<sos>', 'eos': '<eos>', 'unk': '<unk>'}\n\ndef simple_tokenize(s: str) -> List[str]:\n    return s.strip().lower().split()\n\ndef read_split(path: str) -> List[Tuple[List[str], List[str]]]:\n    pairs: List[Tuple[List[str], List[str]]] = []\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            parts = line.rstrip('').split('\t')\n            if len(parts) < 2: continue\n            pairs.append((simple_tokenize(parts[0]), simple_tokenize(parts[1])))\n    return pairs\n\ndef build_vocab(seqs: Iterable[List[str]], max_size: int | None = None) -> Dict[str, int]:\n    from collections import Counter\n    c = Counter();\n    for s in seqs: c.update(s)\n    itms = c.most_common(max_size) if max_size else c.items()\n    stoi = {SPECIAL_TOKENS['pad']:0, SPECIAL_TOKENS['sos']:1, SPECIAL_TOKENS['eos']:2, SPECIAL_TOKENS['unk']:3}\n    for w,_ in itms:\n        if w not in stoi: stoi[w] = len(stoi)\n    return stoi\n\ndef encode(tokens: List[str], stoi: Dict[str,int], add_sos_eos: bool=False) -> List[int]:\n    ids = [stoi.get(t, stoi[SPECIAL_TOKENS['unk']]) for t in tokens]\n    if add_sos_eos: ids = [stoi[SPECIAL_TOKENS['sos']]] + ids + [stoi[SPECIAL_TOKENS['eos']] ]\n    return ids\n\nclass Example:\n    def __init__(self, s: List[int], ti: List[int], to: List[int]): self.src_ids=s; self.tgt_in_ids=ti; self.tgt_out_ids=to\nclass TranslationDataset(Dataset):\n    def __init__(self, pairs, src_stoi, tgt_stoi):\n        self.examples: List[Example] = []\n        for src, tgt in pairs:\n            s = encode(src, src_stoi) + [src_stoi[SPECIAL_TOKENS['eos']]]\n            t = encode(tgt, tgt_stoi, add_sos_eos=True)\n            self.examples.append(Example(s, t[:-1], t[1:]))\n    def __len__(self): return len(self.examples)\n    def __getitem__(self, i): return self.examples[i]\n\ndef collate_pad(batch, pad_id_src: int, pad_id_tgt: int):\n    src_max = max(len(x.src_ids) for x in batch); tgt_max = max(len(x.tgt_in_ids) for x in batch)\n    def pad_to(a, L, pad): return a + [pad]*(L-len(a))\n    src    = torch.tensor([pad_to(x.src_ids,    src_max, pad_id_src) for x in batch])\n    tgt_in = torch.tensor([pad_to(x.tgt_in_ids, tgt_max, pad_id_tgt) for x in batch])\n    tgt_out= torch.tensor([pad_to(x.tgt_out_ids,tgt_max, pad_id_tgt) for x in batch])\n    src_l  = torch.tensor([len(x.src_ids)    for x in batch])\n    tgt_l  = torch.tensor([len(x.tgt_out_ids)for x in batch])\n    return src, src_l, tgt_in, tgt_out, tgt_l\n\ndef compute_perplexity(loss_sum: float, token_count: int) -> float:\n    if token_count==0: return float('inf')\n    try: return float(math.exp(loss_sum/token_count))\n    except OverflowError: return float('inf')\n\ndef corpus_bleu(refs: List[List[str]], hyps: List[List[str]], max_order: int=4, smooth: bool=True) -> float:\n    from collections import Counter\n    def ngrams(t,n): return Counter([tuple(t[i:i+n]) for i in range(len(t)-n+1)])\n    m=[0]*max_order; p=[0]*max_order; rl=0; hl=0\n    for r,h in zip(refs,hyps):\n        rl+=len(r)\n        hl+=len(h)\n        for n in range(1,max_order+1):\n            R=ngrams(r,n); H=ngrams(h,n);\n            m[n-1]+=sum(min(c,H[g]) for g,c in R.items()); p[n-1]+=max(len(h)-n+1,0)\n    prec=[(m[i]+1)/(p[i]+1) if smooth else (m[i]/p[i] if p[i]>0 else 0.0) for i in range(max_order)]\n    geo=math.exp(sum((1/max_order)*math.log(x) for x in prec if x>0)) if min(prec)>0 else 0.0\n    bp=1.0 if hl>rl else math.exp(1-rl/max(1,hl))\n    return float(geo*bp)\n\n@torch.no_grad()\ndef evaluate_nll(loader: DataLoader, model: nn.Module, pad_id_tgt: int, device: torch.device):\n    criterion = nn.CrossEntropyLoss(ignore_index=pad_id_tgt, reduction='sum')\n    model.eval(); tot=0.0; toks=0\n    for src,src_l,tgt_in,tgt_out,tgt_l in loader:\n        src,src_l = src.to(device), src_l.to(device)\n        tgt_in,tgt_out = tgt_in.to(device), tgt_out.to(device)\n        logits = model(src, src_l, tgt_in)\n        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n        tot+=float(loss.item()); toks+=int((tgt_out!=pad_id_tgt).sum().item())\n    return tot, toks\n\n@torch.no_grad()\ndef evaluate_bleu(loader: DataLoader, model: nn.Module, tgt_itos: List[str], sos_id: int, eos_id: int, device: torch.device, max_len: int=100):\n    model.eval(); refs=[]; hyps=[]\n    for src,src_l,tgt_in,tgt_out,tgt_l in loader:\n        src,src_l = src.to(device), src_l.to(device)\n        pred = model.greedy_decode(src, src_l, max_len=max_len, sos_id=sos_id, eos_id=eos_id)\n        for b in range(src.size(0)):\n            ref_ids = tgt_out[b].tolist(); hyp_ids = pred[b].tolist()\n            if eos_id in ref_ids: ref_ids = ref_ids[:ref_ids.index(eos_id)]\n            if eos_id in hyp_ids: hyp_ids = hyp_ids[:hyp_ids.index(eos_id)]\n            refs.append([tgt_itos[i] for i in ref_ids if i!=0])\n            hyps.append([tgt_itos[i] for i in hyp_ids if i!=0 and i!=sos_id])\n    return float(corpus_bleu(refs, hyps))\n","metadata":{"executionInfo":{"elapsed":177,"status":"ok","timestamp":1764770563177,"user":{"displayName":"Gianluca Licciardello","userId":"04706827945424123627"},"user_tz":-60},"id":"pGWPedS7Kkum","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:26:58.250697Z","iopub.execute_input":"2025-12-03T14:26:58.251266Z","iopub.status.idle":"2025-12-03T14:26:58.272264Z","shell.execute_reply.started":"2025-12-03T14:26:58.251245Z","shell.execute_reply":"2025-12-03T14:26:58.271589Z"}},"outputs":[],"execution_count":4},{"id":"yyzL7J-rKkuo","cell_type":"markdown","source":"## 2. Paths and Hyperparameters","metadata":{"id":"yyzL7J-rKkuo"}},{"id":"mQE7FB2tKkup","cell_type":"code","source":"set_seed(42)\ntrain_path = '/kaggle/input/translation/train.txt'\nval_path   = '/kaggle/input/translation/val.txt'\npublic_test_path = '/kaggle/input/translation/public_test.txt'\nif not os.path.exists(public_test_path):\n    alt = 'dataset_splits/test_public.txt'\n    public_test_path = alt if os.path.exists(alt) else public_test_path\nprivate_test_path = 'dataset_splits/private_test.txt'\nsrc_vocab_size = 30000; tgt_vocab_size = 30000\nemb_dim = 256; hid_dim = 512; layers = 1; dropout = 0.1\nbatch_size = 64; epochs = 5; lr = 3e-4; max_decode_len = 100\nsave_dir = 'checkpoints'; os.makedirs(save_dir, exist_ok=True)\nprint('Public test path:', public_test_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2299,"status":"ok","timestamp":1764770571186,"user":{"displayName":"Gianluca Licciardello","userId":"04706827945424123627"},"user_tz":-60},"id":"mQE7FB2tKkup","outputId":"2f1b402a-59c4-40ff-a80e-1d211a5e3a33","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:27:05.836195Z","iopub.execute_input":"2025-12-03T14:27:05.836774Z","iopub.status.idle":"2025-12-03T14:27:05.850847Z","shell.execute_reply.started":"2025-12-03T14:27:05.836750Z","shell.execute_reply":"2025-12-03T14:27:05.849863Z"}},"outputs":[{"name":"stdout","text":"Public test path: /kaggle/input/translation/public_test.txt\n","output_type":"stream"}],"execution_count":5},{"id":"N8EXfFG4Kkup","cell_type":"markdown","source":"## 3. Load Data and Build Vocab","metadata":{"id":"N8EXfFG4Kkup"}},{"id":"sDaqvAawKkuq","cell_type":"code","source":"print('Loading splits...')\ntrain_pairs = read_split(train_path); val_pairs = read_split(val_path); test_pairs = read_split(public_test_path)\nprint(f'Train: {len(train_pairs):,} | Val: {len(val_pairs):,} | Public test: {len(test_pairs):,}')\nsrc_stoi = build_vocab((s for s,_ in train_pairs), max_size=src_vocab_size)\ntgt_stoi = build_vocab((t for _,t in train_pairs), max_size=tgt_vocab_size)\npad_id_src = src_stoi[SPECIAL_TOKENS['pad']]; pad_id_tgt = tgt_stoi[SPECIAL_TOKENS['pad']]\nsos_id = tgt_stoi[SPECIAL_TOKENS['sos']]; eos_id = tgt_stoi[SPECIAL_TOKENS['eos']]\ntrain_ds = TranslationDataset(train_pairs, src_stoi, tgt_stoi); val_ds = TranslationDataset(val_pairs, src_stoi, tgt_stoi); test_ds = TranslationDataset(test_pairs, src_stoi, tgt_stoi)\ncollate = lambda b: collate_pad(b, pad_id_src, pad_id_tgt)\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  collate_fn=collate, num_workers=0)\nval_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\ntest_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\ntgt_itos = [None]*len(tgt_stoi);\nfor w,i in tgt_stoi.items():\n    if 0<=i<len(tgt_itos): tgt_itos[i]=w\nprint('Vocab sizes — src:', len(src_stoi), 'tgt:', len(tgt_stoi))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10386,"status":"ok","timestamp":1764770584494,"user":{"displayName":"Gianluca Licciardello","userId":"04706827945424123627"},"user_tz":-60},"id":"sDaqvAawKkuq","outputId":"2e44e373-5afa-4046-dc6f-c85e99442a01","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:27:14.070833Z","iopub.execute_input":"2025-12-03T14:27:14.071202Z","iopub.status.idle":"2025-12-03T14:27:20.262729Z","shell.execute_reply.started":"2025-12-03T14:27:14.071177Z","shell.execute_reply":"2025-12-03T14:27:20.262031Z"}},"outputs":[{"name":"stdout","text":"Loading splits...\nTrain: 226,997 | Val: 32,428 | Public test: 32,428\nVocab sizes — src: 30004 tgt: 30004\n","output_type":"stream"}],"execution_count":6},{"id":"JKH9evF4Kkus","cell_type":"markdown","source":"## 4. Build Model (Your Playground)\nKeep the forward/greedy_decode contract so evaluation works. Try adding attention, GRU, Transformer, etc.","metadata":{"id":"JKH9evF4Kkus"}},{"id":"QGsYtvErJA3e","cell_type":"code","source":"!cp \"/kaggle/input/translator/pytorch/default/1/model_prova.py\" .\n\nfrom model_prova import Encoder, Decoder, Seq2Seq\nimport sys\nimport os\n\nclass Attention(nn.Module):\n    def __init__(self, hid_dim):\n        super().__init__()\n        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n        self.v = nn.Linear(hid_dim, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs, mask=None):\n        B, S, H = encoder_outputs.size()\n        hidden = hidden.unsqueeze(1).repeat(1, S, 1)\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n        attention = self.v(energy).squeeze(2)\n        if mask is not None:\n            attention = attention.masked_fill(mask == 0, -1e10)\n        return torch.nn.functional.softmax(attention, dim=1)\n\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers>1 else 0.0)\n\n    def forward(self, src, src_lens):\n        emb = self.emb(src)\n        packed = nn.utils.rnn.pack_padded_sequence(emb, src_lens.cpu(), batch_first=True, enforce_sorted=False)\n        out, (h, c) = self.rnn(packed)\n        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n        return out, (h, c)\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.attention = Attention(hid_dim)\n        self.rnn = nn.LSTM(emb_dim + hid_dim, hid_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers>1 else 0.0)\n        self.proj = nn.Linear(emb_dim + hid_dim * 2, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, tgt_in, hidden, encoder_outputs, mask=None):\n        B, T = tgt_in.size()\n        emb = self.dropout(self.emb(tgt_in))\n        outputs = []\n        for t in range(T):\n            emb_t = emb[:, t:t+1, :]\n            h_t = hidden[0][-1]\n            attn_weights = self.attention(h_t, encoder_outputs, mask)\n            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n            rnn_input = torch.cat((emb_t, context), dim=2)\n            out, hidden = self.rnn(rnn_input, hidden)\n            proj_input = torch.cat((emb_t, context, out), dim=2)\n            output = self.proj(proj_input)\n            outputs.append(output)\n        return torch.cat(outputs, dim=1), hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, enc, dec):\n        super().__init__()\n        self.encoder = enc\n        self.decoder = dec\n\n    def create_mask(self, src, src_lens):\n        B, S = src.size()\n        mask = torch.zeros(B, S, device=src.device)\n        for i, length in enumerate(src_lens):\n            mask[i, :length] = 1\n        return mask\n\n    def forward(self, src, src_lens, tgt_in):\n        encoder_outputs, hidden = self.encoder(src, src_lens)\n        mask = self.create_mask(src, src_lens)\n        logits, _ = self.decoder(tgt_in, hidden, encoder_outputs, mask)\n        return logits\n\n    @torch.no_grad()\n    def greedy_decode(self, src, src_lens, max_len, sos_id, eos_id):\n        B = src.size(0)\n        encoder_outputs, hidden = self.encoder(src, src_lens)\n        mask = self.create_mask(src, src_lens)\n        decoder_input = torch.full((B, 1), sos_id, dtype=torch.long, device=src.device)\n        outputs = []\n        for _ in range(max_len):\n            logits, hidden = self.decoder(decoder_input, hidden, encoder_outputs, mask)\n            next_token = logits[:, -1, :].argmax(-1, keepdim=True)\n            outputs.append(next_token)\n            decoder_input = next_token\n        seqs = torch.cat(outputs, dim=1)\n        for i in range(B):\n            row = seqs[i]\n            if (row == eos_id).any():\n                idx = (row == eos_id).nonzero(as_tuple=False)[0].item()\n                row[idx+1:] = eos_id\n        return seqs\n\nencoder = Encoder(len(src_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout)\ndecoder = Decoder(len(tgt_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout)\nmodel = Seq2Seq(encoder, decoder).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7856,"status":"ok","timestamp":1764770655623,"user":{"displayName":"Gianluca Licciardello","userId":"04706827945424123627"},"user_tz":-60},"id":"QGsYtvErJA3e","outputId":"b02aad06-a6c2-4259-8582-55470099d72b","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:27:57.388378Z","iopub.execute_input":"2025-12-03T14:27:57.388720Z","iopub.status.idle":"2025-12-03T14:28:00.609283Z","shell.execute_reply.started":"2025-12-03T14:27:57.388693Z","shell.execute_reply":"2025-12-03T14:28:00.608338Z"}},"outputs":[{"name":"stdout","text":"Total parameters: 58,524,980\n","output_type":"stream"}],"execution_count":8},{"id":"MakmcMeFKkut","cell_type":"markdown","source":"## 5. Train","metadata":{"id":"MakmcMeFKkut"}},{"id":"0vYKfwYZKkut","cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index=pad_id_tgt, reduction='sum')\nfor epoch in range(1, epochs+1):\n    model.train(); tot=0.0; toks=0\n    for src,src_l,tgt_in,tgt_out,tgt_l in train_loader:\n        src,src_l=src.to(device), src_l.to(device); tgt_in,tgt_out=tgt_in.to(device), tgt_out.to(device)\n        optimizer.zero_grad(); logits=model(src, src_l, tgt_in)\n        loss=criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1)); loss.backward();\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); optimizer.step()\n        tot+=float(loss.item()); toks+=int((tgt_out!=pad_id_tgt).sum().item())\n    tr_ppl=compute_perplexity(tot,toks); v_loss,v_toks=evaluate_nll(val_loader, model, pad_id_tgt, device); v_ppl=compute_perplexity(v_loss,v_toks)\n    print(f'Epoch {epoch:02d} | train ppl: {tr_ppl:.2f} | val ppl: {v_ppl:.2f}')\ntorch.save({'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict(), 'epoch': epochs, 'src_stoi': src_stoi, 'tgt_stoi': tgt_stoi, 'model_cfg': {'emb': emb_dim, 'hid': hid_dim, 'layers': layers, 'dropout': dropout}}, os.path.join(save_dir, 'checkpoint_last.pt'))\nprint('Saved checkpoint:', os.path.join(save_dir, 'checkpoint_last.pt'))","metadata":{"id":"0vYKfwYZKkut","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T14:28:08.313279Z","iopub.execute_input":"2025-12-03T14:28:08.313743Z","iopub.status.idle":"2025-12-03T15:23:13.040019Z","shell.execute_reply.started":"2025-12-03T14:28:08.313719Z","shell.execute_reply":"2025-12-03T15:23:13.039324Z"}},"outputs":[{"name":"stdout","text":"Epoch 01 | train ppl: 36.56 | val ppl: 12.04\nEpoch 02 | train ppl: 7.34 | val ppl: 7.10\nEpoch 03 | train ppl: 4.08 | val ppl: 5.81\nEpoch 04 | train ppl: 2.93 | val ppl: 5.32\nEpoch 05 | train ppl: 2.37 | val ppl: 5.14\nSaved checkpoint: checkpoints/checkpoint_last.pt\n","output_type":"stream"}],"execution_count":9},{"id":"z3IeYuoUKkuu","cell_type":"markdown","source":"## 6. Evaluate: Perplexity, BLEU and ROUGE (Public Test)","metadata":{"id":"z3IeYuoUKkuu"}},{"id":"P8I6N6aoKkuu","cell_type":"code","source":"def _lcs_len(x: List[str], y: List[str]) -> int:\n    m, n = len(x), len(y)\n    if m == 0 or n == 0:\n        return 0\n    dp = [0] * (n + 1)\n    for i in range(1, m + 1):\n        prev = 0\n        for j in range(1, n + 1):\n            tmp = dp[j]\n            if x[i - 1] == y[j - 1]:\n                dp[j] = prev + 1\n            else:\n                dp[j] = max(dp[j], dp[j - 1])\n            prev = tmp\n    return dp[n]\n\ndef corpus_rouge_l(refs: List[List[str]], hyps: List[List[str]]) -> float:\n    scores = []\n    for r, h in zip(refs, hyps):\n        if not r or not h:\n            scores.append(0.0)\n            continue\n        L = _lcs_len(r, h)\n        prec = L / len(h)\n        rec  = L / len(r)\n        if prec + rec == 0:\n            f1 = 0.0\n        else:\n            f1 = 2 * prec * rec / (prec + rec)\n        scores.append(f1)\n    return float(sum(scores) / len(scores)) if scores else 0.0\n\n@torch.no_grad()\ndef evaluate_rouge(loader: DataLoader, model: nn.Module, tgt_itos,\n                   sos_id: int, eos_id: int, device: torch.device,\n                   max_len: int = 100) -> float:\n    model.eval()\n    refs, hyps = [], []\n    for src, src_l, tgt_in, tgt_out, tgt_l in loader:\n        src, src_l = src.to(device), src_l.to(device)\n        pred = model.greedy_decode(src, src_l, max_len=max_len,\n                                   sos_id=sos_id, eos_id=eos_id)\n        for b in range(src.size(0)):\n            ref_ids = tgt_out[b].tolist()\n            hyp_ids = pred[b].tolist()\n            if eos_id in ref_ids:\n                ref_ids = ref_ids[:ref_ids.index(eos_id)]\n            if eos_id in hyp_ids:\n                hyp_ids = hyp_ids[:hyp_ids.index(eos_id)]\n            refs.append([tgt_itos[i] for i in ref_ids if i != 0])\n            hyps.append([tgt_itos[i] for i in hyp_ids if i != 0 and i != sos_id])\n    return corpus_rouge_l(refs, hyps)\n\nval_loss, val_tok = evaluate_nll(val_loader, model, pad_id_tgt, device)\nval_ppl = compute_perplexity(val_loss, val_tok)\n\ntst_loss, tst_tok = evaluate_nll(test_loader, model, pad_id_tgt, device)\ntst_ppl = compute_perplexity(tst_loss, tst_tok)\n\nbleu = evaluate_bleu(test_loader, model, tgt_itos,\n                     sos_id=sos_id, eos_id=eos_id,\n                     device=device, max_len=max_decode_len)\n\nrouge = evaluate_rouge(test_loader, model, tgt_itos,\n                       sos_id=sos_id, eos_id=eos_id,\n                       device=device, max_len=max_decode_len)\n\nprint(f'Validation perplexity: {val_ppl:.2f}')\nprint(f'Public test perplexity: {tst_ppl:.2f}')\nprint(f'Public test BLEU:       {bleu*100:.2f}')\nprint(f'Public test ROUGE-L:    {rouge*100:.2f}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110061,"status":"ok","timestamp":1764758905365,"user":{"displayName":"Gianluca Licciardello","userId":"04706827945424123627"},"user_tz":-60},"id":"P8I6N6aoKkuu","outputId":"1b15c3ed-21d4-49c2-e5a8-5ba1d6dd94f2","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:33:12.129938Z","iopub.execute_input":"2025-12-03T15:33:12.130433Z","iopub.status.idle":"2025-12-03T15:39:56.513298Z","shell.execute_reply.started":"2025-12-03T15:33:12.130397Z","shell.execute_reply":"2025-12-03T15:39:56.512461Z"}},"outputs":[{"name":"stdout","text":"Validation perplexity: 5.14\nPublic test perplexity: 5.09\nPublic test BLEU:       30.45\nPublic test ROUGE-L:    57.82\n","output_type":"stream"}],"execution_count":10},{"id":"0tOlGyd61nWA","cell_type":"markdown","source":"## 7. Output Examples","metadata":{"id":"0tOlGyd61nWA"}},{"id":"gjKEyPA01ufN","cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"Showing translation examples from the test set\")\nprint(\"=\"*80 + \"\\n\")\n\nimport random\nrandom.seed(42)\nnum_examples = 10\nsample_indices = random.sample(range(len(test_pairs)), num_examples)\n\nmodel.eval()\n\nfor idx, sample_idx in enumerate(sample_indices, 1):\n    src_tokens, tgt_tokens = test_pairs[sample_idx]\n    \n    src_ids = encode(src_tokens, src_stoi) + [eos_id]\n    src_tensor = torch.tensor([src_ids], device=device)\n    src_lens = torch.tensor([len(src_ids)], device=device)\n    \n    with torch.no_grad():\n        pred_ids = model.greedy_decode(\n            src_tensor, src_lens, \n            max_len=max_decode_len, \n            sos_id=sos_id, \n            eos_id=eos_id\n        )[0].tolist()\n    \n    if eos_id in pred_ids:\n        pred_ids = pred_ids[:pred_ids.index(eos_id)]\n    pred_tokens = [tgt_itos[i] for i in pred_ids if i != 0 and i != sos_id]\n    \n    src_text = ' '.join(src_tokens)\n    ref_text = ' '.join(tgt_tokens)\n    pred_text = ' '.join(pred_tokens)\n    \n    print(f\"[Example {idx}]\")\n    print(f\"Source (EN):     {src_text}\")\n    print(f\"Reference (DE):  {ref_text}\")\n    print(f\"Prediction (DE): {pred_text}\")\n    print(\"-\" * 80)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":222},"id":"gjKEyPA01ufN","executionInfo":{"status":"error","timestamp":1764759546078,"user_tz":-60,"elapsed":24,"user":{"displayName":"Gianluca Licciardello","userId":"04706827945424123627"}},"outputId":"05f5d5eb-f21a-4572-c96f-7656f2a2585b","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:51:36.132899Z","iopub.execute_input":"2025-12-03T15:51:36.133214Z","iopub.status.idle":"2025-12-03T15:51:36.937111Z","shell.execute_reply.started":"2025-12-03T15:51:36.133188Z","shell.execute_reply":"2025-12-03T15:51:36.936610Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nShowing translation examples from the test set\n================================================================================\n\n[Example 1]\nSource (EN):     this book is hers.\nReference (DE):  dieses buch ist das ihrige.\nPrediction (DE): dieses buch ist ihrs.\n--------------------------------------------------------------------------------\n[Example 2]\nSource (EN):     would you like a brandy?\nReference (DE):  möchtet ihr einen brandy?\nPrediction (DE): möchtest du einen brandy?\n--------------------------------------------------------------------------------\n[Example 3]\nSource (EN):     i've never slept in class.\nReference (DE):  ich habe niemals im unterricht geschlafen.\nPrediction (DE): ich habe noch nie in der klasse gesprochen.\n--------------------------------------------------------------------------------\n[Example 4]\nSource (EN):     i'm ready to leave this place.\nReference (DE):  ich bin bereit, von hier zu verschwinden.\nPrediction (DE): ich bin bereit, das zu verlassen.\n--------------------------------------------------------------------------------\n[Example 5]\nSource (EN):     are you afraid of me?\nReference (DE):  hast du angst vor mir?\nPrediction (DE): haben sie angst vor mir?\n--------------------------------------------------------------------------------\n[Example 6]\nSource (EN):     i found them.\nReference (DE):  ich habe sie gefunden.\nPrediction (DE): ich habe sie gefunden.\n--------------------------------------------------------------------------------\n[Example 7]\nSource (EN):     what time will you go to bed tonight?\nReference (DE):  um wieviel uhr gehst du heute abend ins bett?\nPrediction (DE): um wie viel uhr geht ihr heute abend ins bett?\n--------------------------------------------------------------------------------\n[Example 8]\nSource (EN):     he put on an air of innocence.\nReference (DE):  er gab sich den anschein der unschuld.\nPrediction (DE): er hat sich auf den anschein <unk>\n--------------------------------------------------------------------------------\n[Example 9]\nSource (EN):     i have been to the airport to see my father off.\nReference (DE):  ich habe meinen vater gerade am flughafen verabschiedet.\nPrediction (DE): ich bin zum flughafen, um den flughafen zu verabschieden.\n--------------------------------------------------------------------------------\n[Example 10]\nSource (EN):     what a dummy you are!\nReference (DE):  was bist du doch für ein dummkopf!\nPrediction (DE): was für ein <unk> <unk> du?\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":11},{"id":"_oxp96YpKkuu","cell_type":"markdown","source":"## 8. Private Test (Optional)","metadata":{"id":"_oxp96YpKkuu"}},{"id":"tFG7ZQ9zKkuu","cell_type":"code","source":"if os.path.exists(private_test_path):\n    prv_pairs = read_split(private_test_path); prv_ds = TranslationDataset(prv_pairs, src_stoi, tgt_stoi)\n    prv_loader = DataLoader(prv_ds, batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n    prv_loss, prv_tok = evaluate_nll(prv_loader, model, pad_id_tgt, device); prv_ppl = compute_perplexity(prv_loss, prv_tok)\n    prv_bleu = evaluate_bleu(prv_loader, model, tgt_itos, sos_id=sos_id, eos_id=eos_id, device=device, max_len=max_decode_len)\n    print(f'Private test perplexity: {prv_ppl:.2f}')\n    print(f'Private test BLEU:       {prv_bleu*100:.2f}')\nelse:\n    print('Private test split not found at', private_test_path)","metadata":{"id":"tFG7ZQ9zKkuu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764712760709,"user_tz":-60,"elapsed":52,"user":{"displayName":"Gianluca Licciardello","userId":"04706827945424123627"}},"outputId":"1e0f9cf9-b423-4a67-cee2-4e066c88b81d","trusted":true},"outputs":[],"execution_count":null},{"id":"Xe0AQyYkKkuv","cell_type":"markdown","source":"## 9. Export Predictions for ML‑Arena (Optional)","metadata":{"id":"Xe0AQyYkKkuv"}},{"id":"myPVg-CDKkuv","cell_type":"code","source":"@torch.no_grad()\ndef decode_to_lines(loader: DataLoader, model: nn.Module, tgt_itos: List[str], sos_id: int, eos_id: int, device: torch.device, max_len: int) -> List[str]:\n    lines: List[str] = []\n    for src,src_l,tgt_in,tgt_out,tgt_l in loader:\n        src,src_l = src.to(device), src_l.to(device)\n        pred_ids = model.greedy_decode(src, src_l, max_len=max_len, sos_id=sos_id, eos_id=eos_id)\n        for b in range(src.size(0)):\n            hyp = pred_ids[b].tolist()\n            if eos_id in hyp: hyp = hyp[:hyp.index(eos_id)]\n            toks = [tgt_itos[i] for i in hyp if i != 0 and i != sos_id]\n            lines.append(' '.join(toks))\n    return lines\nexport_split = 'private'; export_format = 'tsv'; export_out = 'submissions/private_predictions.tsv'\nos.makedirs(os.path.dirname(export_out) or '.', exist_ok=True)\npairs = read_split(public_test_path if export_split=='public' else private_test_path)\nexp_ds = TranslationDataset(pairs, src_stoi, tgt_stoi); exp_loader = DataLoader(exp_ds, batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\npreds = decode_to_lines(exp_loader, model, tgt_itos, sos_id=sos_id, eos_id=eos_id, device=device, max_len=max_decode_len)\nif export_format=='tsv':\n    with open(export_out, 'w', encoding='utf-8') as f:\n        for i,h in enumerate(preds): f.write(f'{i}\t{h}')\nelif export_format=='jsonl':\n    import json\n    with open(export_out, 'w', encoding='utf-8') as f:\n        for i,h in enumerate(preds): f.write(json.dumps({'id': i, 'hyp': h}, ensure_ascii=False)+'')\nprint(f'Wrote {len(preds)} predictions to {export_out}')\nprint('Adjust if ML‑Arena requires a different schema.')","metadata":{"id":"myPVg-CDKkuv","colab":{"base_uri":"https://localhost:8080/","height":351},"executionInfo":{"status":"error","timestamp":1764712962154,"user_tz":-60,"elapsed":154,"user":{"displayName":"Gianluca Licciardello","userId":"04706827945424123627"}},"outputId":"04e58d3d-b82c-4f1b-842c-55d2adf14bca","trusted":true},"outputs":[],"execution_count":null}]}